name: Solar System Data Pipeline

on:
  workflow_dispatch:  # Allows manual triggering
  schedule:
    # Runs at 8am Central Time (13:00 UTC)
    - cron: '0 13 * * *'

jobs:
  extract-and-load:
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Authenticate using service account key
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}
          export_default_credentials: true

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-cloud-bigquery

      - name: Extract data from API
        id: extract
        run: |
          python fetch_data.py
          # Verify files were created
          if [ ! -f raw_planets.json ] || [ ! -f raw_moons.json ]; then
            echo "Error: Data extraction failed - JSON files not created"
            exit 1
          fi

      - name: Load data to BigQuery
        id: load
        if: steps.extract.outcome == 'success'
        run: |
          python load_data.py

      - name: Clean up temporary files
        if: always()
        run: |
          rm -f raw_planets.json raw_moons.json
